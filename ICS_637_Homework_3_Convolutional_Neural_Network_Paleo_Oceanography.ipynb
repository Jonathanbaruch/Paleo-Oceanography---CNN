{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRg3UQ16Xwj9"
      },
      "outputs": [],
      "source": [
        "# Classifying forams in paleo oceanography Kaggle competition for ICS 637 @University of Hawaii."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlghoI3imA_w",
        "outputId": "24c28837-41cd-4f70-e67f-64f0e1f50273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1mm2tEB05wQwNHP0SySTCp1-BL6G1IHf0\n",
            "From (redirected): https://drive.google.com/uc?id=1mm2tEB05wQwNHP0SySTCp1-BL6G1IHf0&confirm=t&uuid=c81819db-9b69-40a9-9059-fb6ef3e3af8a\n",
            "To: /content/train.zip\n",
            "100%|██████████| 75.2M/75.2M [00:01<00:00, 42.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Cf-yAfSt706w10p5Dij7ppFLo6Se8Ej7\n",
            "To: /content/test.zip\n",
            "100%|██████████| 18.9M/18.9M [00:00<00:00, 44.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NAd3UPTWWxmXdJ9N_Mr3GWgMmO_Aojnn\n",
            "To: /content/train.csv\n",
            "100%|██████████| 283k/283k [00:00<00:00, 78.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Koi9hpUuUwn1swel9QLsEOr_MZKX2LOg\n",
            "To: /content/test.csv\n",
            "100%|██████████| 65.6k/65.6k [00:00<00:00, 45.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download data files from public google drive.\n",
        "# Takes ~1min\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "import gdown\n",
        "\n",
        "file_ids = {'train.zip': '1mm2tEB05wQwNHP0SySTCp1-BL6G1IHf0',\n",
        "            'test.zip': '1Cf-yAfSt706w10p5Dij7ppFLo6Se8Ej7',\n",
        "            'train.csv': '1NAd3UPTWWxmXdJ9N_Mr3GWgMmO_Aojnn',\n",
        "            'test.csv': '1Koi9hpUuUwn1swel9QLsEOr_MZKX2LOg',\n",
        "            }\n",
        "\n",
        "for output, file_id in file_ids.items():\n",
        "  url=f\"https://drive.google.com/uc?id={file_id}\"\n",
        "  gdown.download(url, output, quiet=False)\n",
        "\n",
        "!unzip -q train.zip\n",
        "!unzip -q test.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLRcpahCiuWE"
      },
      "outputs": [],
      "source": [
        "#Import Libraries & Process Data\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class ImageCSVDataset(Dataset):\n",
        "    def __init__(self, image_dir, csv_file, transform=None, test_set=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (string): Directory with all the images.\n",
        "            csv_file (string): Path to the csv file with labels.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.labels = pd.read_csv(csv_file)\n",
        "        self.transform = transform\n",
        "        self.test_set = test_set\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = self.labels.iloc[idx]['filename']  # Assuming image name is in the first column\n",
        "        image = Image.open(f\"{self.image_dir}/{img_name}\").convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.test_set:\n",
        "          return image\n",
        "\n",
        "        label = self.labels.iloc[idx]['label']  # Assuming label is in the second column\n",
        "        return image, label\n",
        "\n",
        "# Define dataset and data loader.\n",
        "image_dir = 'train/'\n",
        "csv_file = 'train.csv'\n",
        "image_dir_test = 'test/'\n",
        "csv_file_test = 'test.csv'\n",
        "\n",
        "# Define transformations (optional)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomRotation(degrees=45),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.01),  # Add Gaussian noise\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.225, 0.225, 0.225])\n",
        "])\n",
        "\n",
        "dataset = ImageCSVDataset(image_dir=image_dir, csv_file=csv_file, transform=transform)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Split dataset: 90% for training, 10% for validation\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoader for validation set:\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcU5rXKxYs_z",
        "outputId": "227661da-701c-481e-f8fc-e9e4d7e6bcfa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54\n",
            "[Epoch 1, Batch 100] Training Loss: 1.529\n",
            "[Epoch 1, Batch 200] Training Loss: 0.842\n",
            "Epoch 1/20, Validation Loss: 0.6736, Validation Accuracy: 80.48%\n",
            "Validation loss decreased, saving model with loss: 0.6736\n",
            "[Epoch 2, Batch 100] Training Loss: 0.685\n",
            "[Epoch 2, Batch 200] Training Loss: 0.665\n",
            "Epoch 2/20, Validation Loss: 0.6141, Validation Accuracy: 82.10%\n",
            "Validation loss decreased, saving model with loss: 0.6141\n",
            "[Epoch 3, Batch 100] Training Loss: 0.570\n",
            "[Epoch 3, Batch 200] Training Loss: 0.606\n",
            "Epoch 3/20, Validation Loss: 0.5037, Validation Accuracy: 83.95%\n",
            "Validation loss decreased, saving model with loss: 0.5037\n",
            "[Epoch 4, Batch 100] Training Loss: 0.536\n",
            "[Epoch 4, Batch 200] Training Loss: 0.537\n",
            "Epoch 4/20, Validation Loss: 0.4640, Validation Accuracy: 84.76%\n",
            "Validation loss decreased, saving model with loss: 0.4640\n",
            "[Epoch 5, Batch 100] Training Loss: 0.480\n",
            "[Epoch 5, Batch 200] Training Loss: 0.495\n",
            "Epoch 5/20, Validation Loss: 0.4465, Validation Accuracy: 86.49%\n",
            "Validation loss decreased, saving model with loss: 0.4465\n",
            "[Epoch 6, Batch 100] Training Loss: 0.453\n",
            "[Epoch 6, Batch 200] Training Loss: 0.403\n",
            "Epoch 6/20, Validation Loss: 0.3762, Validation Accuracy: 88.11%\n",
            "Validation loss decreased, saving model with loss: 0.3762\n",
            "[Epoch 7, Batch 100] Training Loss: 0.410\n",
            "[Epoch 7, Batch 200] Training Loss: 0.382\n",
            "Epoch 7/20, Validation Loss: 0.3662, Validation Accuracy: 88.45%\n",
            "Validation loss decreased, saving model with loss: 0.3662\n",
            "[Epoch 8, Batch 100] Training Loss: 0.383\n",
            "[Epoch 8, Batch 200] Training Loss: 0.403\n",
            "Epoch 8/20, Validation Loss: 0.3441, Validation Accuracy: 88.57%\n",
            "Validation loss decreased, saving model with loss: 0.3441\n",
            "[Epoch 9, Batch 100] Training Loss: 0.374\n",
            "[Epoch 9, Batch 200] Training Loss: 0.384\n",
            "Epoch 9/20, Validation Loss: 0.3263, Validation Accuracy: 88.57%\n",
            "Validation loss decreased, saving model with loss: 0.3263\n",
            "[Epoch 10, Batch 100] Training Loss: 0.392\n",
            "[Epoch 10, Batch 200] Training Loss: 0.361\n",
            "Epoch 10/20, Validation Loss: 0.3196, Validation Accuracy: 90.30%\n",
            "Validation loss decreased, saving model with loss: 0.3196\n",
            "[Epoch 11, Batch 100] Training Loss: 0.351\n",
            "[Epoch 11, Batch 200] Training Loss: 0.376\n",
            "Epoch 11/20, Validation Loss: 0.3150, Validation Accuracy: 90.30%\n",
            "Validation loss decreased, saving model with loss: 0.3150\n",
            "[Epoch 12, Batch 100] Training Loss: 0.373\n",
            "[Epoch 12, Batch 200] Training Loss: 0.352\n",
            "Epoch 12/20, Validation Loss: 0.3341, Validation Accuracy: 89.38%\n",
            "[Epoch 13, Batch 100] Training Loss: 0.368\n",
            "[Epoch 13, Batch 200] Training Loss: 0.374\n",
            "Epoch 13/20, Validation Loss: 0.3277, Validation Accuracy: 89.72%\n",
            "[Epoch 14, Batch 100] Training Loss: 0.383\n",
            "[Epoch 14, Batch 200] Training Loss: 0.367\n",
            "Epoch 14/20, Validation Loss: 0.3383, Validation Accuracy: 89.03%\n",
            "[Epoch 15, Batch 100] Training Loss: 0.367\n",
            "[Epoch 15, Batch 200] Training Loss: 0.356\n",
            "Epoch 15/20, Validation Loss: 0.3236, Validation Accuracy: 88.57%\n",
            "[Epoch 16, Batch 100] Training Loss: 0.359\n",
            "[Epoch 16, Batch 200] Training Loss: 0.354\n",
            "Epoch 16/20, Validation Loss: 0.3419, Validation Accuracy: 88.11%\n",
            "[Epoch 17, Batch 100] Training Loss: 0.375\n",
            "[Epoch 17, Batch 200] Training Loss: 0.341\n",
            "Epoch 17/20, Validation Loss: 0.3280, Validation Accuracy: 89.61%\n",
            "[Epoch 18, Batch 100] Training Loss: 0.356\n",
            "[Epoch 18, Batch 200] Training Loss: 0.363\n",
            "Epoch 18/20, Validation Loss: 0.3252, Validation Accuracy: 89.15%\n",
            "[Epoch 19, Batch 100] Training Loss: 0.352\n",
            "[Epoch 19, Batch 200] Training Loss: 0.360\n",
            "Epoch 19/20, Validation Loss: 0.3100, Validation Accuracy: 90.65%\n",
            "Validation loss decreased, saving model with loss: 0.3100\n",
            "[Epoch 20, Batch 100] Training Loss: 0.364\n",
            "[Epoch 20, Batch 200] Training Loss: 0.361\n",
            "Epoch 20/20, Validation Loss: 0.3381, Validation Accuracy: 88.11%\n",
            "Finished Training with best validation loss: 0.3100\n"
          ]
        }
      ],
      "source": [
        "# Import & Finetune the Resnet18 CNN Model:\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "\n",
        "# Load Resnet18\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Get num classes\n",
        "num_classes = dataset.labels['label'].max()+1  # Get number of unique classes from the dataset\n",
        "print(num_classes)\n",
        "\n",
        "# Modify the final fully connected layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)  # Replace with your number of classes\n",
        "\n",
        "# Freeze early layers (optional) (when false they are frozen)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "# Unfreeze only the first layer\n",
        "#first_layer = list(model.children())[0]  # Get the first layer\n",
        "#for param in first_layer.parameters():\n",
        "#    param.requires_grad = True\n",
        "\n",
        "\n",
        "# Unfreeze later layers (optional)\n",
        "for layer in list(model.children())[-10:]:  # Unfreeze last 3 layers\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "# 4. Define optimizer and loss function\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 5. Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 6. Train the model (similar to previous example)\n",
        "num_epochs = 20\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "best_val_loss = np.inf\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Training loop\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 100 == 99:\n",
        "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Training Loss: {running_loss / 100:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(val_loader, 0):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Save the model if validation loss improves\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f\"Validation loss decreased, saving model with loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    # Step the scheduler to decrease learning rate if needed\n",
        "    scheduler.step()\n",
        "\n",
        "print(f\"Finished Training with best validation loss: {best_val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZOFY-6UGvBi"
      },
      "outputs": [],
      "source": [
        "#Save model with other name (optional)\n",
        "torch.save(model.state_dict(), 'best_model1.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nufCiJXcvSF",
        "outputId": "94688e9a-428a-4544-c87b-d098098076ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-9b529d19e15d>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))  # Load the saved model parameters\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=54, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load best model (optional)\n",
        "\n",
        "model.load_state_dict(torch.load('best_model.pth'))  # Load the saved model parameters\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6JpilrkPBlH"
      },
      "outputs": [],
      "source": [
        "# Use model to make predictions.\n",
        "\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "\n",
        "# Create test dataloader that doesn't have labels.\n",
        "dataset_test = ImageCSVDataset(image_dir=image_dir_test, csv_file=csv_file_test,\n",
        "                               transform=transform, test_set=True)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
        "\n",
        "predictions = []\n",
        "for i, data in enumerate(dataloader_test, 0):\n",
        "    inputs = data.to(device)\n",
        "    outputs = model(inputs)\n",
        "    predictions.append(outputs.argmax(axis=1).detach().cpu().numpy())\n",
        "predictions = np.concatenate(predictions)\n",
        "\n",
        "# Write predictions to a submission file.\n",
        "df_predictions = pd.read_csv(csv_file_test)\n",
        "df_predictions['predictions'] = predictions\n",
        "df_predictions[['id', 'predictions']].to_csv('submission.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
